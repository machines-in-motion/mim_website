<!doctype html>
<html lang="en">
    <head>
            <!-- Metadata, OpenGraph and Schema.org -->
        <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Ludovic Righetti" />

<!-- SEO tags -->
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning to control | Machines in Motion Laboratory</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Learning to control" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is the website of the machines in motion laboratory, a robotics research laboratory at New York University headed by Ludovic Righetti. We conduct fundamental and applied robotics research with a particular focus on autonomous dexterous manipulation and legged locomotion. The website lists the labs activities, publications, group members and latest news." />
<meta property="og:description" content="This is the website of the machines in motion laboratory, a robotics research laboratory at New York University headed by Ludovic Righetti. We conduct fundamental and applied robotics research with a particular focus on autonomous dexterous manipulation and legged locomotion. The website lists the labs activities, publications, group members and latest news." />
<link rel="canonical" href="https://www.machinesinmotion.org/_projects/learning_control.html" />
<meta property="og:url" content="https://www.machinesinmotion.org/_projects/learning_control.html" />
<meta property="og:site_name" content="Machines in Motion Laboratory" />
<meta property="og:image" content="https://www.machinesinmotion.org/projects/learning.jpg" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://www.machinesinmotion.org/projects/learning.jpg" />
<meta property="twitter:title" content="Learning to control" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"This is the website of the machines in motion laboratory, a robotics research laboratory at New York University headed by Ludovic Righetti. We conduct fundamental and applied robotics research with a particular focus on autonomous dexterous manipulation and legged locomotion. The website lists the labs activities, publications, group members and latest news.","headline":"Learning to control","image":"https://www.machinesinmotion.org/projects/learning.jpg","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.machinesinmotion.org/assets/img/mim_logo_black.png"}},"url":"https://www.machinesinmotion.org/_projects/learning_control.html"}</script>
<!-- End Jekyll SEO tag -->



    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.13.1/font/bootstrap-icons.min.css">
    
    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.3.0/css/fontawesome.min.css" integrity="sha384-IfdMaxM7xApqzQmi9UKLIQPSX+440ganmZq+rMGyqDukniVtKl003KdPruUrtXtK" crossorigin="anonymous"> -->
    <!-- <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha384-wi7fdJzLKizoWPKwP8EkF2Sjc0KPB17yM6hFIZBr5olCzdyJkgB0D89mhhqX+FPI" crossorigin="anonymous">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito+Sans" integrity="sha384-WFY3j4Mz0NysE+uREZjbXop0EKpbaguTp4OC9Lv/cer+h3S9D/KCm+W4bGpZ6+Fa" crossorigin="anonymous">

    <!-- stylesheet -->
    <link rel="stylesheet" href="/assets/css/main.css">
    </head>
    <body>
<header>
    <!-- Navigation bar -->
    <nav id="navbar" class="navbar navbar-expand-lg fixed-top">
      <div class="container">
            <a class="navbar-brand" href="/">
                <img style="float: center; height: 60px; padding-right: 5px;" src="/assets/img/mim_logo_black.png" alt="lab logo">
            </a>
        
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                <li class="nav-item">
                    <a class="nav-link " href="/people/">
                        People
                    </a>
                </li>
                
                
                <li class="nav-item">
                    <a class="nav-link " href="/research/">
                        Research
                    </a>
                </li>
                
                
                <li class="nav-item">
                    <a class="nav-link " href="/education/">
                        Education
                    </a>
                </li>
                
                
                <li class="nav-item">
                    <a class="nav-link " href="/publications/">
                        Publications
                    </a>
                </li>
                
                
                <li class="nav-item">
                    <a class="nav-link " href="/joining/">
                        Joining
                    </a>
                </li>
                
                 
            </ul>
        </div>
      </div>
    </nav>
  </header>


        <div class="container" style="padding-bottom: 100px">
            <div class="container">
            <h1 style="padding-bottom: 20px">Learning to control</h1>
            <img src="/assets/img/projects/learning.jpg" class="img-fluid float-md-start mx-auto d-block" alt="Learning to control illustration" style="width:350px; height:auto; padding-left:0px; padding-right:50px;">
            <p>Robots are increasingly equipped with multiple, redundant, sensing modalities (inertial, force, tactile or visual perception) and it remains a significant challenge to efficiently use this information to create robust behaviors in unknown environments. 
Further, robots are seldom capable of learning new or improving known behaviors as they collect more real-world experience.
To address this challenge, we design learning algorithms capable of using multi-modal sensory data to:</p>
<ol>
  <li>learn dynamic models to anticipate the consequences of robot actions</li>
  <li>learn control policies from simulation, trial and error or human demonstrations</li>
  <li>learn cost and value functions to capture desired goals and behaviors</li>
</ol>

<p>Importantly, we test all of our learning algorithms on real robot for
manipulation and locomotion tasks to ensure that they are robust to real noisy sensors and imperfect actuators.</p>

            </div>
            <div class="container" style="padding-top: 50px; padding-left:0px">
            
            <div class="container">
                <h2>Videos</h2>
                <div class="row align-items-top justify-content-around">
                    
                    <div class="col-sm-12 col-md-6" style="padding-top: 20px; padding-bottom: 50px">
                        <style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'>    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/lDPJzpVzLIk" frameborder="0" allowfullscreen></iframe></div>
                    </div>
                    
                    <div class="col-sm-12 col-md-6" style="padding-top: 20px; padding-bottom: 50px">
                        <style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'>    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/P3WAdhZv-Y4" frameborder="0" allowfullscreen></iframe></div>
                    </div>
                    
                    <div class="col-sm-12 col-md-6" style="padding-top: 20px; padding-bottom: 50px">
                        <style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style><div class='embed-container'>    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/qcBwlyZjnRA" frameborder="0" allowfullscreen></iframe></div>
                    </div>
                    
                </div>
            </div>
            
            </div>
            <div class="bib_section" style="padding-left:0px">
            <h2>Selected publications</h2>
            <ol class="bibliography"><li><!-- <span id="Meduri_Zhu_Jordana_Righetti_2023">[1]A. Meduri, H. Zhu, A. Jordana, and L. Righetti, “MPC with Sensor-Based Online Cost Adaptation,” London, UK, May 2023, [Online]. Available at: http://arxiv.org/abs/2209.09451.</span>
<br> -->


A. 

Meduri,



H. 

Zhu,



A. 

Jordana,



L. 

Righetti,



"MPC with Sensor-Based Online Cost Adaptation,"



    in <i>2023 IEEE-RAS International Conference on Robotics and Automation (ICRA)</i>,


 May,
 2023.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Model predictive control is a powerful tool to generate complex motions for robots. However, it often requires solving non-convex problems online to produce rich behaviors, which is computationally expensive and not always practical in real time. Additionally, direct integration of high dimensional sensor data (e.g. RGB-D images) in the feedback loop is challenging with current state-space methods. This paper aims to address both issues. It introduces a model predictive control scheme, where a neural network constantly updates the cost function of a quadratic program based on sensory inputs, aiming to minimize a general non-convex task loss without solving a non-convex problem online. By updating the cost, the robot is able to adapt to changes in the environment directly from sensor measurement without requiring a new cost design. Furthermore, since the quadratic program can be solved efﬁciently with hard constraints, a safe deployment on the robot is ensured. Experiments with a wide variety of reaching tasks on an industrial robot manipulator demonstrate that our method can efﬁciently solve complex non-convex problems with highdimensional visual sensory inputs, while still being robust to external disturbances.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Meduri_Zhu_Jordana_Righetti_2023,
  address = {London, UK},
  title = {MPC with Sensor-Based Online Cost Adaptation},
  url = {http://arxiv.org/abs/2209.09451},
  note = {arXiv:2209.09451 [cs]},
  booktitle = {2023 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Meduri, Avadesh and Zhu, Huaijiang and Jordana, Armand and Righetti, Ludovic},
  year = {2023},
  month = may
}
" data-bs-html="true">
    Bib
</button>


 
<a href="http://arxiv.org/abs/2209.09451">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Model predictive control is a powerful tool to generate complex motions for robots. However, it often requires solving non-convex problems online to produce rich behaviors, which is computationally expensive and not always practical in real time. Additionally, direct integration of high dimensional sensor data (e.g. RGB-D images) in the feedback loop is challenging with current state-space methods. This paper aims to address both issues. It introduces a model predictive control scheme, where a neural network constantly updates the cost function of a quadratic program based on sensory inputs, aiming to minimize a general non-convex task loss without solving a non-convex problem online. By updating the cost, the robot is able to adapt to changes in the environment directly from sensor measurement without requiring a new cost design. Furthermore, since the quadratic program can be solved efﬁciently with hard constraints, a safe deployment on the robot is ensured. Experiments with a wide variety of reaching tasks on an industrial robot manipulator demonstrate that our method can efﬁciently solve complex non-convex problems with highdimensional visual sensory inputs, while still being robust to external disturbances.</p>


<pre>@inproceedings{Meduri_Zhu_Jordana_Righetti_2023,
  address = {London, UK},
  title = {MPC with Sensor-Based Online Cost Adaptation},
  url = {http://arxiv.org/abs/2209.09451},
  note = {arXiv:2209.09451 [cs]},
  booktitle = {2023 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Meduri, Avadesh and Zhu, Huaijiang and Jordana, Armand and Righetti, Ludovic},
  year = {2023},
  month = may
}
</pre> --></li>
<li><!-- <span id="Viereck_Meduri_Righetti_2022">[2]J. Viereck, A. Meduri, and L. Righetti, “ValueNetQP: Learned one-step optimal control for legged locomotion,” in <i>Proceedings of The 4th Annual Learning for Dynamics and Control Conference</i>, Jun. 2022, vol. 168, pp. 931–942, [Online]. Available at: https://proceedings.mlr.press/v168/viereck22a.html.</span>
<br> -->


J. 

Viereck,



A. 

Meduri,



L. 

Righetti,



"ValueNetQP: Learned one-step optimal control for legged locomotion,"



    in <i>Proceedings of The 4th Annual Learning for Dynamics and Control Conference</i>,

 pp. 931–942,
 Jun,
 2022.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Optimal control is a successful approach to generate motions for complex robots, in particular for legged locomotion. However, these techniques are often too slow to run in real time for model predictive control or one needs to drastically simplify the dynamics model. In this work, we present a method to learn to predict the gradient and hessian of the problem value function, enabling fast resolution of the predictive control problem with a one-step quadratic program. In addition, our method is able to satisfy constraints like friction cones and unilateral constraints, which are important for high dynamics locomotion tasks. We demonstrate the capability of our method in simulation and on a real quadruped robot performing trotting and bounding motions.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Viereck_Meduri_Righetti_2022,
  series = {Proceedings of Machine Learning Research},
  title = {ValueNetQP: Learned one-step optimal control for legged locomotion},
  volume = {168},
  url = {https://proceedings.mlr.press/v168/viereck22a.html},
  booktitle = {Proceedings of The 4th Annual Learning for Dynamics and Control Conference},
  publisher = {PMLR},
  author = {Viereck, Julian and Meduri, Avadesh and Righetti, Ludovic},
  year = {2022},
  month = jun,
  pages = {931--942},
  collection = {Proceedings of Machine Learning Research}
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://proceedings.mlr.press/v168/viereck22a.html">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Optimal control is a successful approach to generate motions for complex robots, in particular for legged locomotion. However, these techniques are often too slow to run in real time for model predictive control or one needs to drastically simplify the dynamics model. In this work, we present a method to learn to predict the gradient and hessian of the problem value function, enabling fast resolution of the predictive control problem with a one-step quadratic program. In addition, our method is able to satisfy constraints like friction cones and unilateral constraints, which are important for high dynamics locomotion tasks. We demonstrate the capability of our method in simulation and on a real quadruped robot performing trotting and bounding motions.</p>


<pre>@inproceedings{Viereck_Meduri_Righetti_2022,
  series = {Proceedings of Machine Learning Research},
  title = {ValueNetQP: Learned one-step optimal control for legged locomotion},
  volume = {168},
  url = {https://proceedings.mlr.press/v168/viereck22a.html},
  booktitle = {Proceedings of The 4th Annual Learning for Dynamics and Control Conference},
  publisher = {PMLR},
  author = {Viereck, Julian and Meduri, Avadesh and Righetti, Ludovic},
  year = {2022},
  month = jun,
  pages = {931--942},
  collection = {Proceedings of Machine Learning Research}
}
</pre> --></li>
<li><!-- <span id="Bogdanovic_Khadiv_Righetti_2022">[3]M. Bogdanovic, M. Khadiv, and L. Righetti, “Model-free Reinforcement Learning for Robust Locomotion using Demonstrations from Trajectory Optimization,” <i>Frontiers in Robotics and AI</i>, 2022, doi: 10.3389/frobt.2022.854212.</span>
<br> -->


M. 

Bogdanovic,



M. 

Khadiv,



L. 

Righetti,



"Model-free Reinforcement Learning for Robust Locomotion using Demonstrations from Trajectory Optimization,"



    <i>Frontiers in Robotics and AI</i>,
    
    



 2022.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="We present a general, two-stage reinforcement learning approach to create robust policies that can be deployed on real robots without any additional training using a single demonstration generated by trajectory optimization. The demonstration is used in the first stage as a starting point to facilitate initial exploration. In the second stage, the relevant task reward is optimized directly and a policy robust to environment uncertainties is computed. We demonstrate and examine in detail the performance and robustness of our approach on highly dynamic hopping and bounding tasks on a quadruped robot.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@article{Bogdanovic_Khadiv_Righetti_2022,
  title = {Model-free Reinforcement Learning for Robust Locomotion using Demonstrations from Trajectory Optimization},
  url = {https://arxiv.org/abs/2107.06629},
  doi = {10.3389/frobt.2022.854212},
  journal = {Frontiers in Robotics and AI},
  author = {Bogdanovic, Miroslav and Khadiv, Majid and Righetti, Ludovic},
  year = {2022}
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/2107.06629">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>We present a general, two-stage reinforcement learning approach to create robust policies that can be deployed on real robots without any additional training using a single demonstration generated by trajectory optimization. The demonstration is used in the first stage as a starting point to facilitate initial exploration. In the second stage, the relevant task reward is optimized directly and a policy robust to environment uncertainties is computed. We demonstrate and examine in detail the performance and robustness of our approach on highly dynamic hopping and bounding tasks on a quadruped robot.</p>


<pre>@article{Bogdanovic_Khadiv_Righetti_2022,
  title = {Model-free Reinforcement Learning for Robust Locomotion using Demonstrations from Trajectory Optimization},
  url = {https://arxiv.org/abs/2107.06629},
  doi = {10.3389/frobt.2022.854212},
  journal = {Frontiers in Robotics and AI},
  author = {Bogdanovic, Miroslav and Khadiv, Majid and Righetti, Ludovic},
  year = {2022}
}
</pre> --></li>
<li><!-- <span id="Jordana_Carpentier_Righetti_2021">[4]A. Jordana, J. Carpentier, and L. Righetti, “Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting,” no. arXiv:2106.11712, Jun. 2021, [Online]. Available at: http://arxiv.org/abs/2106.11712.</span>
<br> -->


A. 

Jordana,



J. 

Carpentier,



L. 

Righetti,



"Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting,"



    <i></i>,
    
     no. arXiv:2106.11712,


 Jun,
 2021.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Modeling dynamical systems plays a crucial role in capturing and understanding complex physical phenomena. When physical models are not suﬃciently accurate or hardly describable by analytical formulas, one can use generic function approximators such as neural networks to capture the system dynamics directly from sensor measurements. As for now, current methods to learn the parameters of these neural networks are highly sensitive to the inherent instability of most dynamical systems of interest, which in turn prevents the study of very long sequences. In this work, we introduce a generic and scalable method based on multiple shooting to learn latent representations of indirectly observed dynamical systems. We achieve state-of-the-art performances on systems observed directly from raw images. Further, we demonstrate that our method is robust to noisy measurements and can handle complex dynamical systems, such as chaotic ones.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@article{Jordana_Carpentier_Righetti_2021,
  title = {Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting},
  url = {http://arxiv.org/abs/2106.11712},
  note = {preprint},
  number = {arXiv:2106.11712},
  publisher = {arXiv},
  author = {Jordana, Armand and Carpentier, Justin and Righetti, Ludovic},
  year = {2021},
  month = jun
}
" data-bs-html="true">
    Bib
</button>


 
<a href="http://arxiv.org/abs/2106.11712">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Modeling dynamical systems plays a crucial role in capturing and understanding complex physical phenomena. When physical models are not suﬃciently accurate or hardly describable by analytical formulas, one can use generic function approximators such as neural networks to capture the system dynamics directly from sensor measurements. As for now, current methods to learn the parameters of these neural networks are highly sensitive to the inherent instability of most dynamical systems of interest, which in turn prevents the study of very long sequences. In this work, we introduce a generic and scalable method based on multiple shooting to learn latent representations of indirectly observed dynamical systems. We achieve state-of-the-art performances on systems observed directly from raw images. Further, we demonstrate that our method is robust to noisy measurements and can handle complex dynamical systems, such as chaotic ones.</p>


<pre>@article{Jordana_Carpentier_Righetti_2021,
  title = {Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting},
  url = {http://arxiv.org/abs/2106.11712},
  note = {preprint},
  number = {arXiv:2106.11712},
  publisher = {arXiv},
  author = {Jordana, Armand and Carpentier, Justin and Righetti, Ludovic},
  year = {2021},
  month = jun
}
</pre> --></li>
<li><!-- <span id="Meduri_Khadiv_Righetti_2021">[5]A. Meduri, M. Khadiv, and L. Righetti, “DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain,” Xi’an, China, May 2021, doi: 10.1109/ICRA48506.2021.9562093.</span>
<br> -->


A. 

Meduri,



M. 

Khadiv,



L. 

Righetti,



"DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain,"



    in <i>2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)</i>,


 May,
 2021.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Reactive stepping and push recovery for biped robots is often restricted to ﬂat terrains because of the difﬁculty in computing capture regions for nonlinear dynamic models. In this paper, we address this limitation by proposing a novel 3D reactive stepper, the DeepQ stepper, that can approximately learn the 3D capture regions of both simpliﬁed and full robot dynamic models using reinforcement learning, which can then be used to ﬁnd optimal steps. The stepper can take into account the entire dynamics of the robot, ignored in most reactive steppers, leading to a signiﬁcant improvement in performance. The DeepQ stepper can handle nonconvex terrain with obstacles, walk on restricted surfaces like stepping stones while tracking different velocities, and recover from external disturbances for a constant low computational cost.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Meduri_Khadiv_Righetti_2021,
  address = {Xi’an, China},
  title = {DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain},
  url = {https://arxiv.org/abs/2010.14834},
  doi = {10.1109/ICRA48506.2021.9562093},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Meduri, Avadesh and Khadiv, Majid and Righetti, Ludovic},
  year = {2021},
  month = may
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/2010.14834">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Reactive stepping and push recovery for biped robots is often restricted to ﬂat terrains because of the difﬁculty in computing capture regions for nonlinear dynamic models. In this paper, we address this limitation by proposing a novel 3D reactive stepper, the DeepQ stepper, that can approximately learn the 3D capture regions of both simpliﬁed and full robot dynamic models using reinforcement learning, which can then be used to ﬁnd optimal steps. The stepper can take into account the entire dynamics of the robot, ignored in most reactive steppers, leading to a signiﬁcant improvement in performance. The DeepQ stepper can handle nonconvex terrain with obstacles, walk on restricted surfaces like stepping stones while tracking different velocities, and recover from external disturbances for a constant low computational cost.</p>


<pre>@inproceedings{Meduri_Khadiv_Righetti_2021,
  address = {Xi’an, China},
  title = {DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain},
  url = {https://arxiv.org/abs/2010.14834},
  doi = {10.1109/ICRA48506.2021.9562093},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Meduri, Avadesh and Khadiv, Majid and Righetti, Ludovic},
  year = {2021},
  month = may
}
</pre> --></li>
<li><!-- <span id="Viereck_Righetti_2021">[6]J. Viereck and L. Righetti, “Learning a Centroidal Motion Planner for Legged Locomotion,” Xi’an, China, May 2021, doi: 10.1109/ICRA48506.2021.9562022.</span>
<br> -->


J. 

Viereck,



L. 

Righetti,



"Learning a Centroidal Motion Planner for Legged Locomotion,"



    in <i>2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)</i>,


 May,
 2021.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Whole-body optimizers have been successful at automatically computing complex dynamic locomotion behaviors. However they are often limited to ofﬂine planning as they are computationally too expensive to replan with a high frequency. Simpler models are then typically used for online replanning. In this paper we present a method to generate whole body movements in real-time for locomotion tasks. Our approach consists in learning a centroidal neural network that predicts the desired centroidal motion given the current state of the robot and a desired contact plan. The network is trained using an existing whole body motion optimizer. Our approach enables to learn with few training samples dynamic motions that can be used in a complete whole-body control framework at high frequency, which is usually not attainable with typical full-body optimizers. We demonstrate our method to generate a rich set of walking and jumping motions on a real quadruped robot.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Viereck_Righetti_2021,
  address = {Xi’an, China},
  title = {Learning a Centroidal Motion Planner for Legged Locomotion},
  url = {https://arxiv.org/abs/2011.02818},
  doi = {10.1109/ICRA48506.2021.9562022},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Viereck, Julian and Righetti, Ludovic},
  year = {2021},
  month = may
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/2011.02818">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Whole-body optimizers have been successful at automatically computing complex dynamic locomotion behaviors. However they are often limited to ofﬂine planning as they are computationally too expensive to replan with a high frequency. Simpler models are then typically used for online replanning. In this paper we present a method to generate whole body movements in real-time for locomotion tasks. Our approach consists in learning a centroidal neural network that predicts the desired centroidal motion given the current state of the robot and a desired contact plan. The network is trained using an existing whole body motion optimizer. Our approach enables to learn with few training samples dynamic motions that can be used in a complete whole-body control framework at high frequency, which is usually not attainable with typical full-body optimizers. We demonstrate our method to generate a rich set of walking and jumping motions on a real quadruped robot.</p>


<pre>@inproceedings{Viereck_Righetti_2021,
  address = {Xi’an, China},
  title = {Learning a Centroidal Motion Planner for Legged Locomotion},
  url = {https://arxiv.org/abs/2011.02818},
  doi = {10.1109/ICRA48506.2021.9562022},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Viereck, Julian and Righetti, Ludovic},
  year = {2021},
  month = may
}
</pre> --></li>
<li><!-- <span id="Bechtle_Molchanov_Chebotar_Grefenstette_Righetti_Sukhatme_Meier_2021">[7]S. Bechtle <i>et al.</i>, “Meta-learning via learned loss,” Milan, Jan. 2021, doi: 10.1109/ICPR48806.2021.9412010.</span>
<br> -->


S. 

Bechtle,



A. 

Molchanov,



Y. 

Chebotar,



E. 

Grefenstette,



L. 

Righetti,



G. 

S. 

Sukhatme,



F. 

Meier,



"Meta-learning via learned loss,"



    in <i>25th International Conference on Pattern Recognition</i>,


 Jan,
 2021.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for
“meta-training” such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional informa- tion at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Bechtle_Molchanov_Chebotar_Grefenstette_Righetti_Sukhatme_Meier_2021,
  address = {Milan},
  title = {Meta-learning via learned loss},
  url = {https://arxiv.org/abs/1906.05374},
  doi = {10.1109/ICPR48806.2021.9412010},
  booktitle = {25th International Conference on Pattern Recognition},
  publisher = {IEEE},
  author = {Bechtle, S and Molchanov, A and Chebotar, Y and Grefenstette, E and Righetti, L and Sukhatme, G S and Meier, F},
  year = {2021},
  month = jan
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/1906.05374">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for
“meta-training” such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional informa- tion at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree</p>


<pre>@inproceedings{Bechtle_Molchanov_Chebotar_Grefenstette_Righetti_Sukhatme_Meier_2021,
  address = {Milan},
  title = {Meta-learning via learned loss},
  url = {https://arxiv.org/abs/1906.05374},
  doi = {10.1109/ICPR48806.2021.9412010},
  booktitle = {25th International Conference on Pattern Recognition},
  publisher = {IEEE},
  author = {Bechtle, S and Molchanov, A and Chebotar, Y and Grefenstette, E and Righetti, L and Sukhatme, G S and Meier, F},
  year = {2021},
  month = jan
}
</pre> --></li>
<li><!-- <span id="Bechtle_Hammoud_Rai_Meier_Righetti_2021">[8]S. Bechtle, B. Hammoud, A. Rai, F. Meier, and L. Righetti, “Leveraging Forward Model Prediction Error for Learning Control,” Xi’an, China, May 2021, doi: 10.1109/ICRA48506.2021.9561396.</span>
<br> -->


S. 

Bechtle,



B. 

Hammoud,



A. 

Rai,



F. 

Meier,



L. 

Righetti,



"Leveraging Forward Model Prediction Error for Learning Control,"



    in <i>2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)</i>,


 May,
 2021.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Learning for model based control can be sampleefﬁcient and generalize well, however successfully learning models and controllers that represent the problem at hand can be challenging for complex tasks. Using inaccurate models for learning can lead to sub-optimal solutions that are unlikely to perform well in practice. In this work, we present a learning approach which iterates between model learning and data collection and leverages forward model prediction error for learning control. We show how using the controller’s prediction as input to a forward model can create a differentiable connection between the controller and the model, allowing us to formulate a loss in the state space. This lets us include forward model prediction error during controller learning and we show that this creates a loss objective that signiﬁcantly improves learning on different motor control tasks. We provide empirical and theoretical results that show the beneﬁts of our method and present evaluations in simulation for learning control on a 7 DoF manipulator and an underactuated 12 DoF quadruped. We show that our approach successfully learns controllers for challenging motor control tasks involving contact switching.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Bechtle_Hammoud_Rai_Meier_Righetti_2021,
  address = {Xi’an, China},
  title = {Leveraging Forward Model Prediction Error for Learning Control},
  url = {https://arxiv.org/abs/2011.03859},
  doi = {10.1109/ICRA48506.2021.9561396},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Bechtle, Sarah and Hammoud, Bilal and Rai, Akshara and Meier, Franziska and Righetti, Ludovic},
  year = {2021},
  month = may
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/2011.03859">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Learning for model based control can be sampleefﬁcient and generalize well, however successfully learning models and controllers that represent the problem at hand can be challenging for complex tasks. Using inaccurate models for learning can lead to sub-optimal solutions that are unlikely to perform well in practice. In this work, we present a learning approach which iterates between model learning and data collection and leverages forward model prediction error for learning control. We show how using the controller’s prediction as input to a forward model can create a differentiable connection between the controller and the model, allowing us to formulate a loss in the state space. This lets us include forward model prediction error during controller learning and we show that this creates a loss objective that signiﬁcantly improves learning on different motor control tasks. We provide empirical and theoretical results that show the beneﬁts of our method and present evaluations in simulation for learning control on a 7 DoF manipulator and an underactuated 12 DoF quadruped. We show that our approach successfully learns controllers for challenging motor control tasks involving contact switching.</p>


<pre>@inproceedings{Bechtle_Hammoud_Rai_Meier_Righetti_2021,
  address = {Xi’an, China},
  title = {Leveraging Forward Model Prediction Error for Learning Control},
  url = {https://arxiv.org/abs/2011.03859},
  doi = {10.1109/ICRA48506.2021.9561396},
  booktitle = {2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  author = {Bechtle, Sarah and Hammoud, Bilal and Rai, Akshara and Meier, Franziska and Righetti, Ludovic},
  year = {2021},
  month = may
}
</pre> --></li>
<li><!-- <span id="Bogdanovic_Khadiv_Righetti_2020">[9]M. Bogdanovic, M. Khadiv, and L. Righetti, “Learning Variable Impedance Control for Contact Sensitive Tasks,” <i>IEEE Robotics and Automation Letters</i>, vol. 5, no. 4, pp. 6129–6136, 2020, doi: 10.1109/LRA.2020.3011379.</span>
<br> -->


M. 

Bogdanovic,



M. 

Khadiv,



L. 

Righetti,



"Learning Variable Impedance Control for Contact Sensitive Tasks,"



    <i>IEEE Robotics and Automation Letters</i>,
     vol. 5,
     no. 4,

 pp. 6129–6136,

 2020.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Reinforcement learning algorithms have shown great success in solving different problems ranging from playing video games to robotics. However, they struggle to solve delicate robotic problems, especially those involving contact interactions. Though in principle a policy directly outputting joint torques should be able to learn to perform these tasks, in practice we see that it has difficulty to robustly solve the problem without any given structure in the action space. In this paper, we investigate how the choice of action space can give robust performance in presence of contact uncertainties. We propose learning a policy giving as output impedance and desired position in joint space and compare the performance of that approach to torque and position control under different contact uncertainties. Furthermore, we propose an additional reward term designed to regularize these variable impedance control policies, giving them interpretability and facilitating their transfer to real systems. We present extensive experiments in simulation of both floating and fixed-base systems in tasks involving contact uncertainties, as well as results for running the learned policies on a real system.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@article{Bogdanovic_Khadiv_Righetti_2020,
  title = {Learning Variable Impedance Control for Contact Sensitive Tasks},
  volume = {5},
  url = {https://arxiv.org/abs/1907.07500},
  doi = {10.1109/LRA.2020.3011379},
  number = {4},
  journal = {IEEE Robotics and Automation Letters},
  author = {Bogdanovic, M. and Khadiv, M. and Righetti, L.},
  year = {2020},
  pages = {6129–6136}
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/1907.07500">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Reinforcement learning algorithms have shown great success in solving different problems ranging from playing video games to robotics. However, they struggle to solve delicate robotic problems, especially those involving contact interactions. Though in principle a policy directly outputting joint torques should be able to learn to perform these tasks, in practice we see that it has difficulty to robustly solve the problem without any given structure in the action space. In this paper, we investigate how the choice of action space can give robust performance in presence of contact uncertainties. We propose learning a policy giving as output impedance and desired position in joint space and compare the performance of that approach to torque and position control under different contact uncertainties. Furthermore, we propose an additional reward term designed to regularize these variable impedance control policies, giving them interpretability and facilitating their transfer to real systems. We present extensive experiments in simulation of both floating and fixed-base systems in tasks involving contact uncertainties, as well as results for running the learned policies on a real system.</p>


<pre>@article{Bogdanovic_Khadiv_Righetti_2020,
  title = {Learning Variable Impedance Control for Contact Sensitive Tasks},
  volume = {5},
  url = {https://arxiv.org/abs/1907.07500},
  doi = {10.1109/LRA.2020.3011379},
  number = {4},
  journal = {IEEE Robotics and Automation Letters},
  author = {Bogdanovic, M. and Khadiv, M. and Righetti, L.},
  year = {2020},
  pages = {6129–6136}
}
</pre> --></li>
<li><!-- <span id="Bechtle_Lin_Rai_Righetti_Meier_2019">[10]S. Bechtle, Y. Lin, A. Rai, L. Righetti, and F. Meier, “Curious iLQR: Resolving Uncertainty in Model-based RL,” in <i>Proceedings of the Conference on Robot Learning</i>, Osaka, Japan, Nov. 2019, vol. 100, pp. 162–171, [Online]. Available at: https://arxiv.org/abs/1904.06786.</span>
<br> -->


S. 

Bechtle,



Y. 

Lin,



A. 

Rai,



L. 

Righetti,



F. 

Meier,



"Curious iLQR: Resolving Uncertainty in Model-based RL,"



    in <i>Proceedings of the Conference on Robot Learning</i>,

 pp. 162–171,
 Nov,
 2019.

<br>



<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover" data-bs-content="Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In this work, we propose a model-based reinforcement learning (MBRL) framework that combines Bayesian modeling of the system dynamics with curious iLQR, an iterative LQR approach that considers model uncertainty. During trajectory optimization the curious iLQR attempts to minimize both the task-dependent cost and the uncertainty in the dynamics model. We demonstrate the approach on reaching tasks with 7-DoF manipulators in simulation and on a real robot. Our experiments show that MBRL with curious iLQR reaches desired end-effector targets more reliably and with less system rollouts when learning a new task from scratch, and that the learned model generalizes better to new reaching tasks.">
    Abs
</button>




<button type="button" class="btn_bib btn btn-sm" data-bs-custom-class="abstract-popover" data-bs-toggle="popover"  data-bs-content="@inproceedings{Bechtle_Lin_Rai_Righetti_Meier_2019,
  address = {Osaka, Japan},
  series = {Proceedings of Machine Learning Research},
  title = {Curious iLQR: Resolving Uncertainty in Model-based RL},
  volume = {100},
  url = {https://arxiv.org/abs/1904.06786},
  booktitle = {Proceedings of the Conference on Robot Learning},
  author = {Bechtle, S and Lin, Y and Rai, A and Righetti, L and Meier, F},
  year = {2019},
  month = nov,
  pages = {162–171},
  collection = {Proceedings of Machine Learning Research}
}
" data-bs-html="true">
    Bib
</button>


 
<a href="https://arxiv.org/abs/1904.06786">
<i class="bi bi-link-45deg" style="font-size: 1.5rem; color: #AA5042;"></i>
</a>


<!-- 
<p>Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In this work, we propose a model-based reinforcement learning (MBRL) framework that combines Bayesian modeling of the system dynamics with curious iLQR, an iterative LQR approach that considers model uncertainty. During trajectory optimization the curious iLQR attempts to minimize both the task-dependent cost and the uncertainty in the dynamics model. We demonstrate the approach on reaching tasks with 7-DoF manipulators in simulation and on a real robot. Our experiments show that MBRL with curious iLQR reaches desired end-effector targets more reliably and with less system rollouts when learning a new task from scratch, and that the learned model generalizes better to new reaching tasks.</p>


<pre>@inproceedings{Bechtle_Lin_Rai_Righetti_Meier_2019,
  address = {Osaka, Japan},
  series = {Proceedings of Machine Learning Research},
  title = {Curious iLQR: Resolving Uncertainty in Model-based RL},
  volume = {100},
  url = {https://arxiv.org/abs/1904.06786},
  booktitle = {Proceedings of the Conference on Robot Learning},
  author = {Bechtle, S and Lin, Y and Rai, A and Righetti, L and Meier, F},
  year = {2019},
  month = nov,
  pages = {162–171},
  collection = {Proceedings of Machine Learning Research}
}
</pre> --></li></ol> 
            </div>
        </div>

        
<footer class="fixed-bottom">
  <div class="container text-center">
    &copy; Copyright 2025 Ludovic  Righetti. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with custom theme.

    Last updated: November 03, 2025.
  </div>
</footer>

    <!-- JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>
    
    <script>
        const popoverTriggerList = document.querySelectorAll('[data-bs-toggle="popover"]')
        const popoverList = [...popoverTriggerList].map(popoverTriggerEl => new bootstrap.Popover(popoverTriggerEl))
    </script>
</body>
</html>
