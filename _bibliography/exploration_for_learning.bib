@inproceedings{bechtle_curious_2019,
	address = {Osaka, Japan},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Curious {iLQR}: {Resolving} {Uncertainty} in {Model}-based {RL}},
	volume = {100},
	url = {https://arxiv.org/abs/1904.06786},
	abstract = {Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In this work, we propose a model-based reinforcement learning (MBRL) framework that combines Bayesian modeling of the system dynamics with curious iLQR, an iterative LQR approach that considers model uncertainty. During trajectory optimization the curious iLQR attempts to minimize both the task-dependent cost and the uncertainty in the dynamics model. We demonstrate the approach on reaching tasks with 7-DoF manipulators in simulation and on a real robot. Our experiments show that MBRL with curious iLQR reaches desired end-effector targets more reliably and with less system rollouts when learning a new task from scratch, and that the learned model generalizes better to new reaching tasks.},
	booktitle = {Proceedings of the {Conference} on {Robot} {Learning}},
	author = {Bechtle, S and Lin, Y and Rai, A and Righetti, L and Meier, F},
	month = nov,
	year = {2019},
	keywords = {NYU, conference, website},
	pages = {162--171},
	file = {Bechtle et al. - 2019 - Curious iLQR Resolving Uncertainty in Model-based.pdf:/Users/righetti/Documents/literature/My papers/Bechtle et al. - 2019 - Curious iLQR Resolving Uncertainty in Model-based.pdf:application/pdf}
}


@inproceedings{bogdanovic_learning_2019,
	address = {Macau, China},
	title = {Learning to {Explore} in {Motion} and {Interaction} {Tasks}},
	isbn = {978-1-72814-004-9},
	url = {https://arxiv.org/abs/1908.03731},
	doi = {10.1109/IROS40897.2019.8968584},
	abstract = {Model free reinforcement learning suffers from the high sampling complexity inherent to robotic manipulation or locomotion tasks. Most successful approaches typically use random sampling strategies which leads to slow policy convergence. In this paper we present a novel approach for efficient exploration that leverages previously learned tasks. We exploit the fact that the same system is used across many tasks and build a generative model for exploration based on data from previously solved tasks to improve learning new tasks. The approach also enables continuous learning of improved exploration strategies as novel tasks are learned. Extensive simulations on a robot manipulator performing a variety of motion and contact interaction tasks demonstrate the capabilities of the approach. In particular, our experiments suggest that the exploration strategy can more than double learning speed, especially when rewards are sparse. Moreover, the algorithm is robust to task variations and parameter tuning, making it beneficial for complex robotic problems.},
	booktitle = {{IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Bogdanovic, M and Righetti, L},
	month = nov,
	year = {2019},
	keywords = {NYU, conference, website},
	file = {Bogdanovic and Righetti - 2019 - Learning to Explore in Motion and Interaction Task.pdf:/Users/righetti/Documents/literature/My papers/Bogdanovic and Righetti - 2019 - Learning to Explore in Motion and Interaction Task.pdf:application/pdf}
}