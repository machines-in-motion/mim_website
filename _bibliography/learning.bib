 @inproceedings{Bechtle_Lin_Rai_Righetti_Meier_2019, 
 address={Osaka, Japan}, 
 series={Proceedings of Machine Learning Research}, 
 title={Curious iLQR: Resolving Uncertainty in Model-based RL}, 
 volume={100}, 
 url={https://arxiv.org/abs/1904.06786}, 
 abstract={Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In this work, we propose a model-based reinforcement learning (MBRL) framework that combines Bayesian modeling of the system dynamics with curious iLQR, an iterative LQR approach that considers model uncertainty. During trajectory optimization the curious iLQR attempts to minimize both the task-dependent cost and the uncertainty in the dynamics model. We demonstrate the approach on reaching tasks with 7-DoF manipulators in simulation and on a real robot. Our experiments show that MBRL with curious iLQR reaches desired end-effector targets more reliably and with less system rollouts when learning a new task from scratch, and that the learned model generalizes better to new reaching tasks.}, 
 booktitle={Proceedings of the Conference on Robot Learning}, 
 author={Bechtle, S and Lin, Y and Rai, A and Righetti, L and Meier, F}, 
 year={2019}, 
 month={Nov}, 
 pages={162–171}, 
 collection={Proceedings of Machine Learning Research} 
}
 @inproceedings{Bechtle_Molchanov_Chebotar_Grefenstette_Righetti_Sukhatme_Meier_2021, 
 address={Milan}, 
 title={Meta-learning via learned loss}, 
 url={https://arxiv.org/abs/1906.05374}, 
 DOI={10.1109/ICPR48806.2021.9412010}, 
 abstract={Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for
“meta-training” such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional informa- tion at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at https://sites.google.com/view/mlthree}, 
 booktitle={25th International Conference on Pattern Recognition}, 
 publisher={IEEE}, 
 author={Bechtle, S and Molchanov, A and Chebotar, Y and Grefenstette, E and Righetti, L and Sukhatme, G S and Meier, F}, 
 year={2021}, 
 month={Jan} 
}
 @inproceedings{Bechtle_Hammoud_Rai_Meier_Righetti_2021, 
 address={Xi’an, China}, 
 title={Leveraging Forward Model Prediction Error for Learning Control}, 
 url={https://arxiv.org/abs/2011.03859}, 
 DOI={10.1109/ICRA48506.2021.9561396}, 
 abstract={Learning for model based control can be sampleefﬁcient and generalize well, however successfully learning models and controllers that represent the problem at hand can be challenging for complex tasks. Using inaccurate models for learning can lead to sub-optimal solutions that are unlikely to perform well in practice. In this work, we present a learning approach which iterates between model learning and data collection and leverages forward model prediction error for learning control. We show how using the controller’s prediction as input to a forward model can create a differentiable connection between the controller and the model, allowing us to formulate a loss in the state space. This lets us include forward model prediction error during controller learning and we show that this creates a loss objective that signiﬁcantly improves learning on different motor control tasks. We provide empirical and theoretical results that show the beneﬁts of our method and present evaluations in simulation for learning control on a 7 DoF manipulator and an underactuated 12 DoF quadruped. We show that our approach successfully learns controllers for challenging motor control tasks involving contact switching.}, 
 booktitle={2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)}, 
 publisher={IEEE}, 
 author={Bechtle, Sarah and Hammoud, Bilal and Rai, Akshara and Meier, Franziska and Righetti, Ludovic}, 
 year={2021}, 
 month={May} 
}
 @article{Bogdanovic_Khadiv_Righetti_2020, 
 title={Learning Variable Impedance Control for Contact Sensitive Tasks}, 
 volume={5}, 
 url={https://arxiv.org/abs/1907.07500}, 
 DOI={10.1109/LRA.2020.3011379}, 
 abstract={Reinforcement learning algorithms have shown great success in solving different problems ranging from playing video games to robotics. However, they struggle to solve delicate robotic problems, especially those involving contact interactions. Though in principle a policy directly outputting joint torques should be able to learn to perform these tasks, in practice we see that it has difficulty to robustly solve the problem without any given structure in the action space. In this paper, we investigate how the choice of action space can give robust performance in presence of contact uncertainties. We propose learning a policy giving as output impedance and desired position in joint space and compare the performance of that approach to torque and position control under different contact uncertainties. Furthermore, we propose an additional reward term designed to regularize these variable impedance control policies, giving them interpretability and facilitating their transfer to real systems. We present extensive experiments in simulation of both floating and fixed-base systems in tasks involving contact uncertainties, as well as results for running the learned policies on a real system.}, 
 number={4}, 
 journal={IEEE Robotics and Automation Letters}, 
 author={Bogdanovic, M. and Khadiv, M. and Righetti, L.}, 
 year={2020}, 
 pages={6129–6136} 
}
 @article{Bogdanovic_Khadiv_Righetti_2022, 
 title={Model-free Reinforcement Learning for Robust Locomotion using Demonstrations from Trajectory Optimization}, 
 url={https://arxiv.org/abs/2107.06629}, 
 DOI={10.3389/frobt.2022.854212}, 
 abstract={We present a general, two-stage reinforcement learning approach to create robust policies that can be deployed on real robots without any additional training using a single demonstration generated by trajectory optimization. The demonstration is used in the first stage as a starting point to facilitate initial exploration. In the second stage, the relevant task reward is optimized directly and a policy robust to environment uncertainties is computed. We demonstrate and examine in detail the performance and robustness of our approach on highly dynamic hopping and bounding tasks on a quadruped robot.}, 
 journal={Frontiers in Robotics and AI}, 
 author={Bogdanovic, Miroslav and Khadiv, Majid and Righetti, Ludovic}, 
 year={2022} 
}
 @article{Jordana_Carpentier_Righetti_2021, 
 title={Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting}, 
 url={http://arxiv.org/abs/2106.11712}, 
 abstract={Modeling dynamical systems plays a crucial role in capturing and understanding complex physical phenomena. When physical models are not suﬃciently accurate or hardly describable by analytical formulas, one can use generic function approximators such as neural networks to capture the system dynamics directly from sensor measurements. As for now, current methods to learn the parameters of these neural networks are highly sensitive to the inherent instability of most dynamical systems of interest, which in turn prevents the study of very long sequences. In this work, we introduce a generic and scalable method based on multiple shooting to learn latent representations of indirectly observed dynamical systems. We achieve state-of-the-art performances on systems observed directly from raw images. Further, we demonstrate that our method is robust to noisy measurements and can handle complex dynamical systems, such as chaotic ones.}, 
 note={preprint}, 
 number={arXiv:2106.11712}, 
 publisher={arXiv}, 
 author={Jordana, Armand and Carpentier, Justin and Righetti, Ludovic}, 
 year={2021}, 
 month={Jun} 
}
 @inproceedings{Meduri_Khadiv_Righetti_2021, 
 address={Xi’an, China}, 
 title={DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain}, 
 url={https://arxiv.org/abs/2010.14834}, 
 DOI={10.1109/ICRA48506.2021.9562093}, 
 abstract={Reactive stepping and push recovery for biped robots is often restricted to ﬂat terrains because of the difﬁculty in computing capture regions for nonlinear dynamic models. In this paper, we address this limitation by proposing a novel 3D reactive stepper, the DeepQ stepper, that can approximately learn the 3D capture regions of both simpliﬁed and full robot dynamic models using reinforcement learning, which can then be used to ﬁnd optimal steps. The stepper can take into account the entire dynamics of the robot, ignored in most reactive steppers, leading to a signiﬁcant improvement in performance. The DeepQ stepper can handle nonconvex terrain with obstacles, walk on restricted surfaces like stepping stones while tracking different velocities, and recover from external disturbances for a constant low computational cost.}, 
 booktitle={2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)}, 
 publisher={IEEE}, 
 author={Meduri, Avadesh and Khadiv, Majid and Righetti, Ludovic}, 
 year={2021}, 
 month={May} 
}
 @inproceedings{Meduri_Zhu_Jordana_Righetti_2023, 
 address={London, UK}, 
 title={MPC with Sensor-Based Online Cost Adaptation}, 
 url={http://arxiv.org/abs/2209.09451}, 
 abstract={Model predictive control is a powerful tool to generate complex motions for robots. However, it often requires solving non-convex problems online to produce rich behaviors, which is computationally expensive and not always practical in real time. Additionally, direct integration of high dimensional sensor data (e.g. RGB-D images) in the feedback loop is challenging with current state-space methods. This paper aims to address both issues. It introduces a model predictive control scheme, where a neural network constantly updates the cost function of a quadratic program based on sensory inputs, aiming to minimize a general non-convex task loss without solving a non-convex problem online. By updating the cost, the robot is able to adapt to changes in the environment directly from sensor measurement without requiring a new cost design. Furthermore, since the quadratic program can be solved efﬁciently with hard constraints, a safe deployment on the robot is ensured. Experiments with a wide variety of reaching tasks on an industrial robot manipulator demonstrate that our method can efﬁciently solve complex non-convex problems with highdimensional visual sensory inputs, while still being robust to external disturbances.}, 
 note={arXiv:2209.09451 [cs]}, 
 booktitle={2023 IEEE-RAS International Conference on Robotics and Automation (ICRA)}, 
 publisher={IEEE}, 
 author={Meduri, Avadesh and Zhu, Huaijiang and Jordana, Armand and Righetti, Ludovic}, 
 year={2023}, 
 month={May} 
}
 @inproceedings{Viereck_Meduri_Righetti_2022, 
 series={Proceedings of Machine Learning Research}, 
 title={ValueNetQP: Learned one-step optimal control for legged locomotion}, 
 volume={168}, 
 url={https://proceedings.mlr.press/v168/viereck22a.html}, 
 abstract={Optimal control is a successful approach to generate motions for complex robots, in particular for legged locomotion. However, these techniques are often too slow to run in real time for model predictive control or one needs to drastically simplify the dynamics model. In this work, we present a method to learn to predict the gradient and hessian of the problem value function, enabling fast resolution of the predictive control problem with a one-step quadratic program. In addition, our method is able to satisfy constraints like friction cones and unilateral constraints, which are important for high dynamics locomotion tasks. We demonstrate the capability of our method in simulation and on a real quadruped robot performing trotting and bounding motions.}, 
 booktitle={Proceedings of The 4th Annual Learning for Dynamics and Control Conference}, 
 publisher={PMLR}, 
 author={Viereck, Julian and Meduri, Avadesh and Righetti, Ludovic}, 
 year={2022}, 
 month={Jun}, 
 pages={931--942}, 
 collection={Proceedings of Machine Learning Research} 
}
 @inproceedings{Viereck_Righetti_2021, 
 address={Xi’an, China}, 
 title={Learning a Centroidal Motion Planner for Legged Locomotion}, 
 url={https://arxiv.org/abs/2011.02818}, 
 DOI={10.1109/ICRA48506.2021.9562022}, 
 abstract={Whole-body optimizers have been successful at automatically computing complex dynamic locomotion behaviors. However they are often limited to ofﬂine planning as they are computationally too expensive to replan with a high frequency. Simpler models are then typically used for online replanning. In this paper we present a method to generate whole body movements in real-time for locomotion tasks. Our approach consists in learning a centroidal neural network that predicts the desired centroidal motion given the current state of the robot and a desired contact plan. The network is trained using an existing whole body motion optimizer. Our approach enables to learn with few training samples dynamic motions that can be used in a complete whole-body control framework at high frequency, which is usually not attainable with typical full-body optimizers. We demonstrate our method to generate a rich set of walking and jumping motions on a real quadruped robot.}, 
 booktitle={2021 IEEE-RAS International Conference on Robotics and Automation (ICRA)}, 
 publisher={IEEE}, 
 author={Viereck, Julian and Righetti, Ludovic}, 
 year={2021}, 
 month={May} 
}
